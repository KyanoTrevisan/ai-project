{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Model Development for Racing Game Automation\n",
    "## Overview\n",
    "This report provides a comprehensive overview of the development and implementation of an AI model to automate gameplay in Trackmania which is free on steam and a game which isn't too heavy on the system so we would be able to work on our laptops along with our desktops. This report will walk you through how we approached the project for both the OpenCV and the deep learning portions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "- Emre Ekici\n",
    "- Salih Ekici\n",
    "- Kyano Trevisan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encountered Problems and Solutions\n",
    "\n",
    "During the development of the scripts, particularly those involving OpenCV for game steering, we encountered a few challenges. Here's an overview of these issues and the solutions we implemented:\n",
    "\n",
    "### 1. Difficulty with Timing in OpenCV Script\n",
    "- **Problem**: One of the main difficulties was related to the timing of key inputs. Since we were using a keyboard for inputs instead of a controller, we couldn't input driving angles directly. This limitation made it challenging to precisely control the steering in the game.\n",
    "- **Solution**: We adjusted the script to better handle the timing of key presses. This involved fine-tuning the duration for which each key was held down and the intervals between key presses. By experimenting with these timings, we were able to achieve more nuanced control, even with the binary nature of keyboard inputs.\n",
    "\n",
    "### 2. Issue with pyautogui in TrackMania\n",
    "- **Problem**: Initially, we tried using `pyautogui` for simulating keyboard inputs. However, we found that `pyautogui` was not effective in the context of the game TrackMania.\n",
    "- **Solution**: After some research and testing, we switched to `pydirectinput`. This library is specifically designed for direct input in games and works well with games like TrackMania. `PyDirectInput` provided more reliable and consistent input simulation compared to `pyautogui`, which solved the issue we were facing.\n",
    "\n",
    "### General Tips for Similar Projects\n",
    "- **Experiment with Timing**: Timing of key presses is crucial, especially when simulating analog input (like steering angles) with digital means (like keyboard presses). It often requires trial and error to get it right.\n",
    "- **Choose the Right Library for Input Simulation**: Not all libraries work the same in every context, especially in gaming environments. If one library doesnâ€™t work (like `pyautogui` in our case), try others (like `pydirectinput`).\n",
    "- **Document Adjustments**: Keep track of the adjustments you make, especially when fine-tuning timings or switching libraries. This documentation can be invaluable for troubleshooting and future development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Script Descriptions\n",
    "Each script in our project serves a specific purpose in the AI model's development:\n",
    "\n",
    "1. **open_cv.py**: This script utilizes OpenCV for processing screen captures from the game. It detects lane lines and calculates the driving angle and speed, essential for understanding the car's position and movement on the track.\n",
    "\n",
    "2. **gather_play_data.py**: It focuses on collecting gameplay data. This script captures screen data while driving the car manually and logs corresponding control actions. This data forms the basis for training the AI model.\n",
    "\n",
    "3. **create_model.py**: This script is responsible for constructing and training the deep learning model. It includes steps for loading data, preprocessing, setting up the neural network architecture, and training the model with data augmentation techniques.\n",
    "\n",
    "4. **use_model.py**: The final script uses the trained model to automate gameplay. It makes real-time decisions for controlling the car in the game, effectively replacing manual input with AI-driven responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV\n",
    "\n",
    "Script: `open_cv.py`\n",
    "\n",
    "#### Automated Game Steering with OpenCV and PyDirectInput\n",
    "\n",
    "Here, we explore an automated steering system for a game using Python libraries like OpenCV, PyDirectInput, and MSS. The script captures a portion of the game screen, processes the image to detect lines (which represent the road in the game), and then calculates the steering angle to control the game character or vehicle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pydirectinput as pdi\n",
    "import mss\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: open_cv\n",
    "The `open_cv` function takes a screenshot of the specified game area, processes it to detect lines, and calculates the steering angle based on these lines. It uses computer vision techniques to understand the game's environment and make decisions accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take in the screenshot, process it, find the lines and determine the input\n",
    "def open_cv(game_area): \n",
    "    # Intialize a variable to use to determine input\n",
    "    steering_angle = 0\n",
    "\n",
    "    with mss.mss() as sct:\n",
    "        screencap = sct.grab(game_area)\n",
    "    \n",
    "    # Convert screencap to a numpy array and process it to detect line visibility\n",
    "    screencap_np = np.array(screencap)\n",
    "    screencap_gray = cv2.cvtColor(screencap_np, cv2.COLOR_RGB2GRAY)\n",
    "    screencap_blur = cv2.GaussianBlur(screencap_gray, (5, 5), 0)\n",
    "    screencap_canny = cv2.Canny(screencap_blur, threshold1=100, threshold2=300)\n",
    "\n",
    "    # Use the houghlinesp algorithm to detect lines from our processed image\n",
    "\n",
    "    lines = cv2.HoughLinesP(screencap_canny, 1, np.pi/180, threshold=100, minLineLength=150, maxLineGap=15)\n",
    "    line_image = np.zeros_like(screencap_canny)\n",
    "\n",
    "    # Initialize lists to store the left and right lines\n",
    "    left_line_x = []\n",
    "    right_line_x = []\n",
    "\n",
    "    # Grab the wanted region of the screencap \n",
    "    y1 = line_image.shape[0]\n",
    "    y2 = int(y1 * 0.6)\n",
    "\n",
    "    # Check for detected lines\n",
    "    if lines is not None:\n",
    "        # Iterate through every line and calculate the slop for the lines\n",
    "        for line in lines:\n",
    "            for x1, y1, x2, y2 in line:\n",
    "                slope = (y2 - y1) / (x2 - x1) if (x2 - x1) != 0 else float('inf')\n",
    "                # Assign line to the left if slope is negative\n",
    "                if slope < 0:\n",
    "                    left_line_x.extend([x1, x2])\n",
    "                # Assign line to the right if slope is positive\n",
    "                else:\n",
    "                    right_line_x.extend([x1, x2])\n",
    "                cv2.line(line_image, (x1, int(y1)), (x2, int(y2)), (255, 0, 0), 5)\n",
    "    # Grab the average of the left lines and the right lines\n",
    "    left_x_avg = np.mean(left_line_x) if left_line_x else 0\n",
    "    right_x_avg = np.mean(right_line_x) if right_line_x else line_image.shape[1]\n",
    "\n",
    "    # Find the center of the 2 lines\n",
    "    mid_x = (left_x_avg + right_x_avg) / 2\n",
    "\n",
    "    # Define the placement of the car\n",
    "    car_pos = line_image.shape[1] / 2  \n",
    "\n",
    "    # Define deviation to find steering angle\n",
    "    deviation = car_pos - mid_x\n",
    "\n",
    "    # Define steering angle to determine inputs\n",
    "    steering_angle = deviation / line_image.shape[1] * 100  \n",
    "\n",
    "\n",
    "    # Check steering angle to determine the input\n",
    "    # High steering angle = more time pressed down\n",
    "    if 7 > steering_angle > 2:\n",
    "        pdi.keyDown('left')\n",
    "        pdi.keyDown('w')\n",
    "        pdi.keyUp('left')\n",
    "        pdi.keyUp('w')\n",
    "    elif -7 < steering_angle < -2:\n",
    "        pdi.keyDown('right')\n",
    "        pdi.keyDown('w')\n",
    "        pdi.keyUp('right')\n",
    "        pdi.keyUp('w')\n",
    "    elif -2 <= steering_angle <= 2:\n",
    "        pdi.keyDown('w')\n",
    "        time.sleep(0.1)\n",
    "        pdi.keyUp('w')\n",
    "        key_input = 2\n",
    "    elif steering_angle < -7:\n",
    "        pdi.keyDown('w')\n",
    "        pdi.keyDown('right')\n",
    "        time.sleep(0.1)\n",
    "        pdi.keyUp('w')\n",
    "        pdi.keyUp('right')\n",
    "    elif 7 <= steering_angle:\n",
    "        pdi.keyDown('w')\n",
    "        pdi.keyDown('left')\n",
    "        time.sleep(0.1)\n",
    "        pdi.keyUp('w')\n",
    "        pdi.keyUp('left')\n",
    "    else:\n",
    "        pdi.keyDown('w')\n",
    "        time.sleep(.1)\n",
    "        pdi.keyUp('w')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inside the open_cv function\n",
    "\n",
    "1. **Initialization**: The function begins by initializing the steering angle and capturing the game screen using the MSS library.\n",
    "2. **Image Processing**: The captured screen is converted to a grayscale image, blurred, and edges are detected using the Canny method from OpenCV.\n",
    "3. **Line Detection**: It then uses the HoughLinesP method to detect lines in the processed image.\n",
    "4. **Line Analysis**: The function analyzes these lines to differentiate between left and right lines of the road.\n",
    "5. **Steering Angle Calculation**: Based on the position of these lines, it calculates a steering angle.\n",
    "6. **Control Inputs**: Depending on the steering angle, it simulates keyboard inputs using the PyDirectInput library to control the game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # The game is placed in the top left corner of the screen\n",
    "    game_area = {\"left\": 0, \"top\": 270, \"width\": 970, \"height\": 200}\n",
    "    # Time to switch over to the Trackmania window\n",
    "    time.sleep(2)\n",
    "    while True:\n",
    "        time.sleep(.1)\n",
    "        screen_capture = open_cv(game_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Script: `gather_play_data.py`\n",
    "\n",
    "#### Game Screen Capture and Key Press Logging\n",
    "\n",
    "This notebook outlines a Python script that captures a specified area of the game screen and logs key presses. The script uses libraries like OpenCV, MSS, and Keyboard. It's designed to save screenshots of the game and log the corresponding key presses in a CSV file for further analysis or machine learning purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mss\n",
    "import time\n",
    "import keyboard\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: capture_and_save_screen\n",
    "This function captures the specified area of the screen and saves the screenshot to a file. It uses MSS for screen capturing and OpenCV for saving the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_and_save_screen(game_area, file_path):\n",
    "    with mss.mss() as sct:\n",
    "        # Capture the screen at the specified game area\n",
    "        screencap = sct.grab(game_area)\n",
    "        screencap = np.array(screencap)\n",
    "\n",
    "        # Save the captured image\n",
    "        cv2.imwrite(file_path, screencap)\n",
    "\n",
    "    return screencap, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: log_key_press\n",
    "The `log_key_press` function logs the key pressed along with the filename of the screenshot. It writes this data to a CSV file, facilitating the pairing of screen states with user inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_key_press(key, file_path, log_file):\n",
    "    with open(log_file, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([file_path, key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Script\n",
    "The main script sets up the game area for screen capture, initializes directories and files for saving screenshots and logs, and enters a loop to continuously capture the screen and log key presses. It also handles the creation of a CSV file for logging and sets up an OpenCV window to display the captured screen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    game_area = {\"left\": 0, \"top\": 270, \"width\": 970, \"height\": 200}\n",
    "    img_directory = \"screenshots\"\n",
    "    log_file = \"key_log.csv\"\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    if not os.path.exists(img_directory):\n",
    "        os.makedirs(img_directory)\n",
    "\n",
    "    # Initialize csv file for logging\n",
    "    if not os.path.exists(log_file):\n",
    "        with open(log_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"image\", \"key\"])\n",
    "\n",
    "    time.sleep(5)\n",
    "    img_count = 0\n",
    "\n",
    "    while True:\n",
    "        img_count += 1\n",
    "        file_path = os.path.join(img_directory, f\"image_{img_count}.png\")\n",
    "\n",
    "        # Capture the screen and save it\n",
    "        screen_capture, saved_file_path = capture_and_save_screen(game_area, file_path)\n",
    "\n",
    "        # Display the captured screen\n",
    "        cv2.imshow(\"Game Screen\", screen_capture)\n",
    "\n",
    "        # Check for arrow key presses and log them\n",
    "        key_pressed = None\n",
    "        if keyboard.is_pressed('d'):\n",
    "            key_pressed = 'up&right'\n",
    "        elif keyboard.is_pressed('a'):\n",
    "            key_pressed = 'up&left'\n",
    "        elif keyboard.is_pressed('w'):\n",
    "            key_pressed = 'up'    \n",
    "        \n",
    "        if key_pressed:\n",
    "            print(key_pressed)\n",
    "            log_key_press(key_pressed, saved_file_path, log_file)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the OpenCV window\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Training\n",
    "\n",
    "Script: `create_model.py`\n",
    "\n",
    "#### Image Classification with TensorFlow and Keras\n",
    "\n",
    "This notebook demonstrates the process of building an image classification model using TensorFlow and Keras. It involves loading image paths and labels from a CSV file, preprocessing the images, and training a Convolutional Neural Network (CNN) for classification. The dataset is divided into training and validation sets, and the model is trained to classify images based on key presses logged during a gaming session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Desktop\\Downloads\\all ai files\\aivenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Process Dataset\n",
    "\n",
    "The dataset consists of image paths and their corresponding key press labels. The dataset is balanced by sampling an equal number of images for each key press and then split into training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"key_log.csv\"\n",
    "df = pd.read_csv(log_file)\n",
    "group_sizes = df.groupby('key').size()\n",
    "print(group_sizes)\n",
    "sampled_df = df.groupby('key').apply(lambda x: x.sample(n=min(group_sizes))).reset_index(drop=True)\n",
    "train_df, val_df = train_test_split(sampled_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Data Preprocessing\n",
    "\n",
    "ImageDataGenerator is used for data augmentation and preprocessing. The images are rescaled, and various transformations are applied to the training data. Validation data is only rescaled and not augmented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "input_shape = (224, 224, 3)  \n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load and preprocess the training data with data augmentation\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='image',\n",
    "    y_col='key',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',  \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load and preprocess the validation data without data augmentation\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='image',\n",
    "    y_col='key',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Print label tags for indices\n",
    "print(\"Label Tags for Indices in Training Generator:\")\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network (CNN) Model\n",
    "\n",
    "A CNN model is defined using TensorFlow Keras. The model consists of convolutional layers, max pooling layers, and dense layers. It is compiled and trained on the dataset, and the accuracy and loss are monitored on both the training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=20, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Trained Model\n",
    "\n",
    "After training, the model is saved to a file for future use, such as in applications or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelmain.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Usage in Game Automation\n",
    "\n",
    "Script: `use_model.py`\n",
    "\n",
    "#### Real-time Game Control using TensorFlow Model\n",
    "\n",
    "This notebook demonstrates the application of a pre-trained TensorFlow model for real-time game control. The model predicts the necessary key presses based on the game screen captured in real-time. The script uses MSS for screen capturing, TensorFlow for loading and making predictions with the model, and PyDirectInput for simulating key presses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mss\n",
    "import pydirectinput as pdi\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Pre-trained Model\n",
    "\n",
    "The pre-trained TensorFlow model is loaded. This model has been trained to classify screen captures into different categories, each corresponding to a specific key press or combination of key presses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Desktop\\Downloads\\all ai files\\aivenv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Desktop\\Downloads\\all ai files\\aivenv\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('modelmain.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Game Area and Class Labels\n",
    "\n",
    "The area of the game screen to be captured is defined. Additionally, a dictionary mapping class indices to their respective labels (key presses) is set up. This mapping will be used to interpret the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_area = {\"left\": 0, \"top\": 270, \"width\": 970, \"height\": 200}\n",
    "class_labels = {0: 'up', 1: 'up&left', 2: 'up&right'}  # Map class indices to labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: read_screen\n",
    "\n",
    "The `read_screen` function captures a specified area of the game screen, preprocesses the image to match the input requirements of the model, and returns the processed image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_screen():\n",
    "    with mss.mss() as sct:\n",
    "        # Capture the screen at the specified game area\n",
    "        screencap = sct.grab(game_area)\n",
    "        screencap_np = np.array(screencap)\n",
    "\n",
    "    # Discard the alpha channel and convert to RGB\n",
    "    screencap_rgb = screencap_np[:, :, :3]\n",
    "\n",
    "    # Resize to match the input size of the ResNet50 model (224x224)\n",
    "    screencap_resized = tf.image.resize(screencap_rgb, (224, 224))\n",
    "\n",
    "    # Add a batch dimension to the input data\n",
    "    return np.expand_dims(screencap_resized, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Script for Game Control\n",
    "\n",
    "In the main script, the screen is continuously captured and fed into the model to predict the necessary key presses. Based on the model's predictions, corresponding key presses are simulated in real-time to control the game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        prediction_probabilities = model.predict(read_screen())\n",
    "        predicted_class_index = np.argmax(prediction_probabilities)\n",
    "        predicted_class_label = class_labels[predicted_class_index]\n",
    "        \n",
    "        if predicted_class_label:\n",
    "            if '&' in predicted_class_label:\n",
    "                parts = predicted_class_label.split('&')\n",
    "                pdi.keyDown(parts[1])\n",
    "                time.sleep(0.1)\n",
    "                pdi.keyUp(parts[1])\n",
    "                pdi.keyDown(parts[0])\n",
    "                pdi.keyUp(parts[0])\n",
    "            else:\n",
    "                pdi.keyDown(predicted_class_label)\n",
    "                time.sleep(0.1)\n",
    "                pdi.keyUp(predicted_class_label)\n",
    "        \n",
    "        print(\"Predicted Label:\", predicted_class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook encapsulates a journey through automating game control using a combination of technologies like OpenCV, TensorFlow, and PyDirectInput. We navigated various challenges, from image processing and machine learning model training to real-time game interaction and key press simulation.\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Interdisciplinary Integration**: This project illustrates the power of integrating different fieldsâ€”computer vision, machine learning, and game automation. It shows how they can come together to create an innovative solution.\n",
    "2. **Problem-Solving and Adaptability**: We encountered and overcame several challenges. Adjusting the timing of simulated key inputs and switching to a more suitable library for game control are testaments to the importance of problem-solving and adaptability in software development.\n",
    "3. **Practical Applications of AI**: The project offers a glimpse into the practical applications of AI and machine learning. It demonstrates how AI can interact with and control other software systems (like video games) in real-time.\n",
    "\n",
    "### Final Thoughts\n",
    "This project is not just about automating a game; We learned about what can be achieved when we bridge the gap between AI and real-world applications. It's a stepping stone towards more complex and nuanced AI interactions in various domains. As we continue to push the boundaries of what's possible, the learnings from such projects will be invaluable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
